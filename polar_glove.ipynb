{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embeddings and normalizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = glove2word2vec('../data/embeddings/glove.6B/glove.6B.300d.txt', 'gensim_glove_300d.txt') ## needed for glove embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_300d.txt\", binary=False) ## needed for only glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_norm_embedding(model, output_path):\n",
    "    temp_file = open(output_path,'wb')\n",
    "    temp_file.write(str.encode(str(len(model.vocab))+' '+str(model.vector_size)+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        temp_file.write(model[each_word]/linalg.norm(model[each_word]))\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "    \n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_norm_embedding(model_glove,'glove_norm_300.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('glove_norm_300.mod',binary=True) # ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = model_gn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on the size of the antonym vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_antonym = 1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    antonymy_vector.append(current_model[each_word_pair[0]]- current_model[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Dimension Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scipy.spatial.distance import cosine as scipy_cosine\n",
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = defaultdict(dict)\n",
    "for index_1, each_dim1 in enumerate(tqdm(t1)):\n",
    "    for index_2, each_dim2 in enumerate(t1):\n",
    "        dimension_similarity_matrix[index_1][index_2] = abs(1-scipy_cosine(each_dim1, each_dim2))\n",
    "        \n",
    "        \n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "        \n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "    \n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "    \n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "    \n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    \n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "#             print(each_dim)\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        print(test_count,min_dim)\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "#         print(working_list.shape, len(final_list))\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ORTHOGONAL DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_antonym = 1443\n",
    "orthogonal_antonymy_vector =np.array(select_subset_dimension(antonymy_vector, num_antonym))  \n",
    "print(orthogonal_antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the RANDOM DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_antonym_vector = [i for i in range(len(antonymy_vector))]\n",
    "random.shuffle(random_antonym_vector)\n",
    "print(len(random_antonym_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the MAXIMUM VARIANCE DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = antonymy_vector.shape[0]   \n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "for each_word in tqdm(current_model.vocab):\n",
    "    total_words += 1\n",
    "\n",
    "    new_vector = np.matmul(variance_antonymy_vector_inverse,current_model[each_word])\n",
    "    \n",
    "    embedding_matrix.append(new_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_list = []\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "for each_dimension in tqdm(range(embedding_matrix.shape[1])):\n",
    "    variance_list.append(np.var(embedding_matrix[:,each_dimension]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del variance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation to polar space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "    embedding_size = current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model.vocab), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "    \n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model.vocab))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model.vocab))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model.vocab):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "\n",
    "        \n",
    "        \n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard normal transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal_dist_model(model, new_filename):\n",
    "    embedding_matrix = []\n",
    "    embedding_vocab = []\n",
    "\n",
    "    temp_file = open(new_filename,'wb')\n",
    "    temp_file.write(str.encode(str(model.vectors.shape[0])+' '+str(model.vectors.shape[1])+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        embedding_matrix.append(model[each_word])\n",
    "        embedding_vocab.append(each_word)\n",
    "    \n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    print('The shape of embedding matrix is {}'.format(embedding_matrix.shape))\n",
    "    \n",
    "    norm_embedding_matrix = (embedding_matrix - embedding_matrix.mean(0))/ embedding_matrix.std(0)\n",
    "    \n",
    "    for word_counter, each_word in enumerate(tqdm(embedding_vocab)):\n",
    "#         assert each_word==embedding_vocab[word_counter],'Not matching!!!'\n",
    "        \n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        new_vector = norm_embedding_matrix[word_counter]\n",
    "        temp_file.write(new_vector)\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "        \n",
    "    del embedding_matrix\n",
    "    del embedding_vocab\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the task score for different dimension size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../output/polar_glove_embeddings', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dim = 500 # Number of POLAR dimenions\n",
    "\n",
    "for method_name, antonym_vector_method in [\n",
    "        ('rand_antonym_', random_antonym_vector),\n",
    "        ('orthogonal_antonymy_', orthogonal_antonymy_vector),\n",
    "        ('variance_antonymy_', variance_antonymy_vector)]:\n",
    "    embedding_path = f'../output/polar_glove_embeddings/{method_name}'+str(curr_dim)+'.bin'\n",
    "    generate_embedding_path(current_model, embedding_path,True,antonym_vector_method, curr_dim)\n",
    "\n",
    "    print('loading the model')\n",
    "    temp_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "    print('loading done..')\n",
    "\n",
    "    std_nrml_embedding_path = f'../output/polar_glove_embeddings/{method_name}gl_'+str(curr_dim)+'_StdNrml.bin'\n",
    "    standard_normal_dist_model(temp_model, std_nrml_embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
